{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba73cbfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:34:52.364894Z",
     "start_time": "2023-06-13T07:34:50.450361Z"
    }
   },
   "outputs": [],
   "source": [
    "# HuggingFace_SA_1.ipynb - 3.에서 사용한 라이브러리 그대로 가져옴 \n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    roc_auc_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score)\n",
    "from transformers import (\n",
    "    AdamW, \n",
    "    AutoModelForSequenceClassification, \n",
    "    DataCollatorWithPadding, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc4ba2c",
   "metadata": {},
   "source": [
    "# 1. Class weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191e3d0",
   "metadata": {},
   "source": [
    "## 1) Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa860c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:34:52.367613Z",
     "start_time": "2023-06-13T07:34:52.366078Z"
    }
   },
   "outputs": [],
   "source": [
    "data_name = \"sent_merge\"\n",
    "checkpoint = \"skt/kobert-base-v1\"\n",
    "seed = 7353\n",
    "train_proportion = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5f1301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:34:56.182799Z",
     "start_time": "2023-06-13T07:34:52.368322Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8e80763ffbac1bee\n",
      "Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-8e80763ffbac1bee/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff)\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 로드\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = KoBERTTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# dataset load\n",
    "dataset = load_dataset('csv', data_files={'train': f'../data_split/{data_name}_train.csv',\n",
    "                                          'test': f'../data_split/{data_name}_test.csv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9207f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:34:56.186547Z",
     "start_time": "2023-06-13T07:34:56.183581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'label'],\n",
       "        num_rows: 43853\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'label'],\n",
       "        num_rows: 10964\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf64315",
   "metadata": {},
   "source": [
    "## 2) Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d0f673",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:35:03.982874Z",
     "start_time": "2023-06-13T07:34:56.187358Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-8e80763ffbac1bee/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-7d46d2613c1bc41a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-8e80763ffbac1bee/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-05259dc511dece38.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-8e80763ffbac1bee/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-6b41264358b44366.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-8e80763ffbac1bee/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-9830456025cb629c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-8e80763ffbac1bee/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-0711df7d795adcf4.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-8e80763ffbac1bee/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-6562507e0eb6d1c8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-8e80763ffbac1bee/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-78bcddde5c8466bf.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-8e80763ffbac1bee/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-00e9d9f678cd987f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop rows with NA - Done\n",
      "Remove SP - Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cca17b022384662aaa450e55f3dc414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39255835b404e0cbe326baa0b500a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-8e80763ffbac1bee/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff/cache-09f689d63cb09e97.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize - Done\n",
      "Data Split - Done\n"
     ]
    }
   ],
   "source": [
    "# 1) 결측값 있으면 삭제\n",
    "dataset = dataset.filter(lambda row: pd.notnull(row[\"id\"]))\n",
    "dataset = dataset.filter(lambda row: pd.notnull(row[\"text\"]))\n",
    "dataset = dataset.filter(lambda row: pd.notnull(row[\"label\"]))\n",
    "print(\"Drop rows with NA - Done\")\n",
    "\n",
    "# 2) 특수문자 삭제\n",
    "def remove_sp(example):\n",
    "    example[\"text\"]=re.sub(r'[^0-9|ㄱ-ㅎ|ㅏ-ㅣ|가-힣| ]+', '', str(example[\"text\"]))\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(remove_sp)\n",
    "print(\"Remove SP - Done\")\n",
    "\n",
    "# 3) 토크나이징\n",
    "tokenizer = KoBERTTokenizer.from_pretrained(checkpoint, truncation_side = 'right')\n",
    "\n",
    "def tokenize_fn(dataset):\n",
    "    tokenized_batch = tokenizer(dataset[\"text\"],\n",
    "                                padding=True,\n",
    "                                truncation=True,\n",
    "                                max_length = 512)\n",
    "    return tokenized_batch\n",
    "\n",
    "dataset = dataset.map(tokenize_fn, batched=True) # 여러 텍스트가 포함된 하나의 배치 단위로 인코딩\n",
    "print(\"Tokenize - Done\")\n",
    "\n",
    "# 4) train / eval /test split\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=seed).select(range(0,math.floor(len(dataset[\"train\"])*train_proportion)))\n",
    "eval_dataset = dataset[\"train\"].shuffle(seed=seed).select(range(math.floor(len(dataset[\"train\"])*train_proportion), len(dataset[\"train\"])))\n",
    "test_dataset = dataset[\"test\"]\n",
    "print(\"Data Split - Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce6c9a",
   "metadata": {},
   "source": [
    "## 3) 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54615d8f",
   "metadata": {},
   "source": [
    "- [TrainingArguments](https://huggingface.co/docs/transformers/v4.19.2/en/main_classes/trainer#transformers.TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd96876e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:35:04.019184Z",
     "start_time": "2023-06-13T07:35:03.983538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_count: 1\n",
      "device 0 capability (8, 6)\n",
      "device 0 name NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# GPU / CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(\"device_count: {}\".format(device_count))\n",
    "    for device_num in range(device_count):\n",
    "        print(\"device {} capability {}\".format(\n",
    "            device_num,\n",
    "            torch.cuda.get_device_capability(device_num)))\n",
    "        print(\"device {} name {}\".format(\n",
    "            device_num, \n",
    "            torch.cuda.get_device_name(device_num)))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"no cuda device\")\n",
    "    \n",
    "num_gpus = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04267f1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:35:04.159663Z",
     "start_time": "2023-06-13T07:35:04.020106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.8477,   0.4418,   0.5646,   0.8285, 128.9790, 182.7202,   1.7690])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# weights 계산 \n",
    "train_labels = np.array(train_dataset[\"label\"])\n",
    "class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(train_labels), y = train_labels)\n",
    "weights = torch.tensor(class_weights, dtype = torch.float)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9ee1706",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:35:04.162896Z",
     "start_time": "2023-06-13T07:35:04.160460Z"
    }
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5, # config\n",
    "    weight_decay=0.1, # config\n",
    "    adam_beta1=0.9, # config\n",
    "    adam_beta2=0.9, # config\n",
    "    adam_epsilon=1.5e-06, # config\n",
    "    num_train_epochs=10,\n",
    "    max_steps=-1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,  # config\n",
    "    warmup_steps=0,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    no_cuda=num_gpus <= 0,\n",
    "    seed=seed,\n",
    "    fp16=True,\n",
    "    eval_steps = 50,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=True,\n",
    "    metric_for_best_model=\"objective\", # f1 + acc\n",
    "    report_to=\"none\",\n",
    "    skip_memory_metrics=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03bf4804",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:35:04.181963Z",
     "start_time": "2023-06-13T07:35:04.163525Z"
    }
   },
   "outputs": [],
   "source": [
    "# train() method를 호출할 때마다 모델 초기화\n",
    "# 이렇게 하면 train 중간에 중지했다가 다시 run해도 오류나지 않습니다;\n",
    "def _model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint,\n",
    "        num_labels = 7,\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40010960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:35:04.197615Z",
     "start_time": "2023-06-13T07:35:04.182697Z"
    }
   },
   "outputs": [],
   "source": [
    "# 평가 매트릭 정의\n",
    "def _compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1_weighted = f1_score(labels, predictions, average = 'weighted')\n",
    "    acc_weighted = accuracy_score(labels, predictions)\n",
    "    return {\"acc_weighted\": acc_weighted, \"f1_weighted\": f1_weighted, \"objective\": acc_weighted + f1_weighted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6f33ba2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:35:04.208857Z",
     "start_time": "2023-06-13T07:35:04.199131Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_collator 정의\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # 배치단위로 패딩 수행할 수 있도록 해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c2b0b08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-13T07:35:04.222249Z",
     "start_time": "2023-06-13T07:35:04.210839Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'id', 'input_ids', 'label', 'text', 'token_type_ids'],\n",
       "    num_rows: 30697\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer클래스를 상속받아 새로운 CustomTrainer 클래스를 만들고, 그 안의 compute_loss 함수를 새로 작성\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        weight = weights.to(device)\n",
    "        loss_fct = torch.nn.MultiMarginLoss(weight=weight) # loss function for 다중분류\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=_model_init(),\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=_compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
