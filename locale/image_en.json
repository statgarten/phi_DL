{
    "en":{

        "sub_title":"Image Classification",

        "expander_image":"Image Upload",

        "add_class_button":"Add Class",
        "del_class_button":"Delete Class",

        "class_name":"name of class",
        "class_num":"class",

        "train_image_preview":"Pick preview image list",
        "train_image_prev":"← previous",
        "train_image_next":"next →",

        "upload_train_image":"Please upload a train image",

        "expander_train":"Model Train",

        "HP_learning_rate":"Learning rate :question:",
        "HP_batch_size":"Btach size :question:",
        "HP_epoch":"Epoch :question:",
        "HP_optimizer":"Optimizer :question:",

        "ex_learning_rate":"Learning rate refers to the amount or steps trained in Mahcine learning. \n\nLearning rate baseline value \nHow you set the value of the learning rate determines the ML results. Only by setting the optimal learning rate will you get the results you want in the end. If the value of the learning rate is not appropriate, overflow can also occur. In short, if the learning rate is too large, you won't be able to reduce the errors that occur during training. On the other hand, too low a learning rate is not a good thing. If the learning rate is too low, the ML process will take too long and the number of error values to validate will be too high, causing the machine learning to stall. In short, a high learning rate will produce faster results, but may not produce enough error values or may cause overflows; conversely, a low learning rate will produce slower results and may produce too many error values, causing the execution process to stall. Therefore, it's important to find the right learning rate value.\n\nLearning rate default value \nIn general, you can try values like 0.1, 0.01, 0.001, etc.",
        "ex_batch_size":"Batch size refers to the amount of data to use when updating parameters during model training. \n\nBatch size examples \nLet's take the example of how humans learn through problem solving. The batch size is the same as deciding how many questions to run and grade at once. For example, if there are 100 questions in total, and you solve and grade 20 of them, the batch size is 20. As people solve and grade problems, they learn why they got it wrong or how they got it right. The same is true for deep learning models. The Optimizer updates the parameter by calculating the error (conf. loss function) between the model's predicted value and the actual correct answer using data as large as the batch size.\n\nBatch size range \nIf the batch size is too large, the amount of data that needs to be processed at once increases, slowing down training and risking out-of-memory issues. Conversely, if the batch size is too small, the weights will be updated based on less data, and these updates will occur frequently, making the training unstable.",
        "ex_epoch":"Epoch refers to the number of times the entire dataset was trained.\n\nEpoch example \nLet's return to our example of a person studying from a question book. An epoch is the number of times you've completed all of the questions in a set from start to finish, including grading. Some people have solved the entire book once, while others have solved it 3, 5, or even 10 times. An epoch is how many times you've solved a book of questions like this. This means that if you have 10 epochs, you have trained your model on training dataset A 10 times.\n\nEpoch range \nAs you increase the number of epochs, the probability of finding a good parameter increases (i.e., the loss value goes down) because you are training with different random weights. However, if you increase the epoch too much, you increase the likelihood that it will overfit that training dataset and make poor predictions on other data.",
        "ex_optimizer":"An optimizer is a deep learning machine that should learn to be as wrong as possible. The function that tells you how wrong you are is the loss function. The learning objective is to find the minimum value of the loss function. The process of finding the minimum value is called optimization, and the algorithm that does it is called an optimizer.\n\nOpimizer Types \n1. Adam \nA method like Adagrad or RMSProp where each parameter has a different size update. Adam's intuition is not to roll fast just because he can jump the local minima, but to slow down carefully to explore the minima.\n\n2. SGD\nSGD doesn't update its weights and biases with the entire input data, but only a subset of it. Randomly extract a batch size of data from the entire x,y data, which we call a mini batch. This not only speeds up learning, but also saves memory. \n\n3. Adagrad \nAdagrad can change the learning rate for each parameter and each step. A type of second-order optimization algorithm, computed on the derivative of the loss function.",        

        "explanation_text":"if you click hyper parameter name or ?, you can see hyper parameter explanation",
        "explanation_title":"Explanation",
        
        "training_model_button":"Training Model",
        "training_model_spinner":"Model is training. Don't click other button",
        "training_model_complete":"Train completed : )",
        "training_model_error":"Train error : (",

        "expander_test":"Model Test",

        "upload_test_image":"Upload a test image",
        
        "prediction_result":"Prediction result",
        
        "please_train_model":"Please Train Model : <",

        "model_download":"Download Model"

        }
}
