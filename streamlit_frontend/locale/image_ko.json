{
  "ko":{

      "sub_title":"이미지 분류",

      "expander_image":"이미지 업로드",

      "add_class_button":"클래스 추가",
      "del_class_button":"클래스 삭제",

      "class_name":"클래스 이름",
      "class_num":"클래스",

      "train_image_preview":"미리보기 할 이미지를 선택해주세요",
      "train_image_prev":"← 이전",
      "train_image_next":"다음 →",

      "upload_train_image":"학습 이미지를 업로드해주세요",

      "expander_train":"모델 학습",

      "HP_learning_rate":"학습률 :question:",
      "HP_batch_size":"배치 사이즈 :question:",
      "HP_epoch":"에포크 :question:",
      "HP_optimizer":"최적화 함수 :question:",

      "ex_learning_rate":"Learning rate 란 \n한국에서 학습률이라고 불리는 Mahcine learning에서 training 되는 양 또는 단계를 의미합니다. \n\nLearning rate 기준 값 \nLearning rate(학습률)의 값을 어떻게 설정하느냐에 따라서 ML 결과가 달라집니다. 최적의 학습률을 설정해야지만 최종적으로 원하는 결과를 산출해낼 수 있습니다. Learning rate의 값이 적합하지 않을 경우, Overflow가 발생할 수도 있습니다. 한마디로 학습률이 너무 크면 Training 과정에서 발생하는 오류를 줄이지 못한다는 것입니다. 반면에 학습률이 너무 낮다고 해서 좋지만은 않습니다. 학습률이 너무 낮을 경우에는 ML 과정이 오래 걸리고 검증해내는 오류 값이 너무 많아져 Machine learning이 멈출 수가 있습니다. 한마디로 Learning rate가 높으면 산출되는 결과 속도가 빨라지지만 오류 값을 제대로 산출해내지 못하거나 오버플로우가 발생할 수 있고, 반대로 Learning rate가 너무 낮으면 산출되는 결과 속도가 느려지고 오류 값이 너무 많아져 실행 과정 자체가 멈출 수 있습니다. 따라서 적합한 Learning rate 값을 찾는 것이 중요합니다. \n\nLearning rate 초기값 \n일반적으로 0.1, 0.01, 0.001 등의 값을 시도해 볼 수 있습니다.",
      "ex_batch_size":"Batch size 란 \nBatch 크기는 모델 학습 중 parameter를 업데이트할 때 사용할 데이터 개수를 의미합니다. \n\nBatch size 예시 \n사람이 문제 풀이를 통해 학습해 나가는 과정을 예로 들어보겠습니다. Batch 크기는 몇 개의 문제를 한 번에 쭉 풀고 채점할지를 결정하는 것과 같습니다. 예를 들어, 총 100개의 문제가 있을 때, 20개씩 풀고 채점한다면 Batch 크기는 20입니다. 사람은 문제를 풀고 채점을 하면서 문제를 틀린 이유나 맞춘 원리를 학습합니다. 딥러닝 모델 역시 마찬가지입니다. Batch 크기만큼 데이터를 활용해 모델이 예측한 값과 실제 정답 간의 오차(conf. 손실함수)를 계산하여 Optimizer가 parameter를 업데이트합니다. \n\nBatch size 범위 \nBatch size가 너무 큰 경우 한 번에 처리해야 할 데이터의 양이 많아지므로, 학습 속도가 느려지고, 메모리 부족 문제가 발생할 위험이 있습니다. 반대로, Batch size가 너무 작은 경우 적은 데이터를 대상으로 가중치를 업데이트하고, 이 업데이트가 자주 발생하므로, 훈련이 불안정해집니다. ",
      "ex_epoch":"Epoch 란 \n'에포크'라고 읽고 전체 데이터셋을 학습한 횟수를 의미합니다. \n\nEpoch 예시 \n사람이 문제집으로 공부하는 상황을 다시 예로 들어보겠습니다. epoch는 문제집에 있는 모든 문제를 처음부터 끝까지 풀고, 채점까지 마친 횟수를 의미합니다. 문제집 한 권 전체를 1번 푼 사람도 있고, 3번, 5번, 심지어 10번 푼 사람도 있습니다. epoch는 이처럼 문제집 한 권을 몇 회 풀었는지를 의미합니다. 즉 epoch가 10회라면, 학습 데이터 셋 A를 10회 모델에 학습시켰다는 것 입니다. \n\nEpoch 범위 \nEpoch를 높일수록, 다양한 무작위 가중치로 학습을 해보므로, 적합한 파라미터를 찾을 확률이 올라갑니다.(즉, 손실 값이 내려가게 됩니다.) 그러나, 지나치게 epoch를 높이게 되면, 그 학습 데이터셋에 과적합(Overfitting)되어 다른 데이터에 대해선 제대로 된 예측을 하지 못할 가능성이 올라갑니다.",
      "ex_optimizer":"Optimizer 란 \n딥러닝은 학습시 최대한 틀리지 않는 방향으로 학습해야 합니다. 얼마나 틀리는지(Loss)을 알게 하는 함수가 loss function(손실함수)입니다. loss function의 최솟값을 찾는 것을 학습 목표로 합니다. 최솟값을 찾아가는 과정이 최적화(Optimization), 이를 수행하는 알고리즘이 최적화 알고리즘(Optimizer)입니다. \n\nOpimizer 종류 \n1. Adam \nAdagrad나 RMSProp처럼 각 파라미터마다 다른 크기의 업데이트를 진행하는 방법입니다. Adam의 직관은 local minima를 뛰어넘을 수 있다는 이유만으로 빨리 굴러가는 것이 아닌, minima의 탐색을 위해 조심스럽게 속도를 줄이고자 하는 것입니다. \n\n2. SGD \nSGD는 전체 입력 데이터로 가중치와 편향이 업데이트되는 것이 아니라, 그 안의 일부 데이터만 이용합니다. 전체 x, y 데이터에서 랜덤하게 배치 사이즈만큼 데이터를 추출하는데, 이를 미니 배치(mini batch)라고 합니다. 이를 통해 학습 속도를 빠르게 할 수 있을 뿐만 아니라 메모리도 절약할 수 있습니다. \n\n3. Adagrad \nAdagrad는 각 파라미터와 각 단계마다 학습률을 변경할 수 있습니다. second-order 최적화 알고리즘의 유형으로, 손실함수의 도함수에 대해 계산됩니다.",   

      "explanation_text":"각 하이퍼파라미터의 이름 또는 ? 를 클릭하시면 설명을 보실 수 있습니다.",
      "explanation_title":"하이퍼파라미터 설명",
      
      "training_model_button":"모델 학습",
      "training_model_spinner":"모델 학습중입니다. 다른 버튼을 누르지 마세요.",
      "training_model_complete":"학습 완료 : )",
      "training_model_error":"학습 실패 : (",

      "expander_test":"모델 테스트",

      "upload_test_image":"테스트 이미지를 업로드 해주세요",
      
      "prediction_result":"예측 결과",
      
      "please_train_model":"모델을 학습시켜 주세요 : <",

      "model_download":"모델 다운로드"

      }
}
