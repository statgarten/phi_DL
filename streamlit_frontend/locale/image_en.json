{
    "en":{

        "sub_title":"Image Classification",

        "expander_image":"Image Upload",

        "add_class_button":"Add Class",
        "del_class_button":"Delete Class",

        "class_name":"name of class",
        "class_num":"class",

        "train_image_preview":"Pick preview image list",
        "train_image_prev":"← previous",
        "train_image_next":"next →",

        "upload_train_image":"Please upload a train image",

        "expander_train":"Model Train",

        "HP_learning_rate":"Learning rate :question:",
        "HP_batch_size":"Btach size :question:",
        "HP_epoch":"Epoch :question:",
        "HP_optimizer":"Optimizer :question:",

        "ex_learning_rate":"Learning Rate\n\nThe learning rate is a hyperparameter that determines how quickly the model learns. If the learning rate is too high, we may overshoot the optimal solution. Conversely, if the learning rate is too low, the learning process can be very slow. \n\nTherefore, finding the appropriate learning rate is important. As an initial value, you can try values like 0.1, 0.01, or 0.001 according to the rule of thumb.",
        "ex_batch_size":"Batch Size\n\nBatch size in deep learning indicates the number of data trained at once. For instance, if you train 1000 data with a batch size of 100 and an epoch of 1, the model will learn 10 times using 100 data at a time. \n\nThe larger the batch size, the more computational efficiency increases. However, too large a batch size can harm the model's ability to generalize and requires more memory. Conversely, too small a batch size can make learning unstable and lower the computer's computational efficiency.",
        "ex_epoch":"Epoch\n\nAn epoch refers to the number of times the entire dataset is trained. For instance, if you train 1000 data with a batch size of 100 and 5 epochs, the model will update its weights 50 times (50 steps) per epoch, using 100 data at a time for learning.\n\nThe more epochs, the more opportunities the model has to learn. However, too many epochs may lead to overfitting, a phenomenon where the model fits the training data too well, thus decreasing its predictive performance on new data.",
        "ex_optimizer":"Optimizer\n\nAn optimizer is an algorithm that decides how to update the weights of a deep learning model. The fundamental objective is to minimize the value of the loss function. The smaller the value of the loss function, the closer the model's prediction is to the actual value.\n\nAdam: This is one of the most widely used optimizers currently. Adam combines momentum and RMSprop to update weights, enabling generally fast and effective learning.\n\nSGD (Stochastic Gradient Descent): This is the most basic optimizer that uses the gradient calculated from each batch to update weights. However, SGD may tend to oscillate while searching for the optimal solution.\n\nAdagrad: This is an improved version compared to SGD, dynamically adjusting the learning rate. This prevents the learning rate from decreasing too rapidly and works better for sparse data.",        

        "explanation_text":"if you click hyperparameter name or ?, you can see hyperparameter explanation",
        "explanation_title":"Explanation",
        
        "training_model_button":"Training Model",
        "training_model_spinner":"Model is training. Don't click other button",
        "training_model_complete":"Train completed : )",
        "training_model_error":"Train error : (",

        "expander_test":"Model Test",

        "upload_test_image":"Upload a test image",
        
        "prediction_result":"Prediction result",
        
        "please_train_model":"Please Train Model : <",

        "model_download":"Download Model"

        }
}
